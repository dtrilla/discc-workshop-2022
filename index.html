<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>Workshop on Data Integrity and Secure Cloud Computing (DISCC 2022)</title>

    <!-- css -->
    <link rel="stylesheet" href="bower_components/bootstrap/dist/css/bootstrap.min.css">
    <link rel="stylesheet" href="bower_components/ionicons/css/ionicons.min.css">
    <link rel="stylesheet" href="assets/css/main.css">
	<style>
		.popover {
		        max-width: 80% !important;
		    }
	</style>
</head>
<body data-spy="scroll" data-target="#site-nav">
    <nav id="site-nav" class="navbar navbar-fixed-top navbar-custom">
        <div class="container">
            <div class="navbar-header">

                <!-- logo -->
                <div class="site-branding">
                    <a class="logo" href="index.html">
                        
                        <!-- logo image  -->
                        <!-- <img src="assets/images/logo.png" alt="Logo"> -->
                        DISCC 2022
                    </a>
                </div>

                <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar-items" aria-expanded="false">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>

            </div><!-- /.navbar-header -->

            <div class="collapse navbar-collapse" id="navbar-items">
                <ul class="nav navbar-nav navbar-right">

                    <!-- navigation menu -->
                    <li class="active"><a data-scroll href="#about">About</a></li>
                    <li><a data-scroll href="#about">Call for Contributions</a></li>
                    <li><a data-scroll href="#speakers">Speakers</a></li>                  
                    <li><a data-scroll href="#program">Program</a></li>                  
                    <li><a data-scroll href="#org">Organizers</a></li>                  
                
                </ul>
            </div>
        </div><!-- /.container -->
    </nav>

    <header id="site-header" class="site-header valign-center"> 
        <div class="intro">

            <p>October 2<sup>nd</sup> 2022</p>
            
            <h1 class="display-1">DISCC 2022</h1>
            <h1>1<sup>st</sup> Workshop on Data Integrity and Secure Cloud Computing</h1>
            
            <p>IBM Research</p>
            
            <a class="btn btn-white" data-scroll href="#about">Contribute Now</a>

            <a class="btn btn-white" data-scroll href="#registration">Register Now</a>
        
        </div>
    </header>

    <section id="about" class="section about">
        <div class="container">
            <div class="row">
                <div class="col-sm-6">

                    <h3 class="section-title">About</h3>
                    
                    <p>
                    The <b>1<sup>st</sup> Workshop on Data Integrity and Secure Cloud Computing (DISCC 2022)</b> will be held on Thursday October 27th
                    , 2022 <strong>in person at IBM Thomas J. Watson, Yorktown Heights</strong>. This event is a full-day workshop  that provides a forum for invited
					students in a broad range of fields covering all aspects of architectures for the  future of computing. Invited students
					are expected to showcase their work and interact with their peers and members of  the IBM Research community.<br><br>

                    The topics covered by FOCA 2022 include but are not limited to:
                    </p>
                    
                    <ul class="list-arrow-right">
                        <li>Architectures for artificial intelligence / machine learning.</li>
                        <li>Security- and reliability-aware architectures.</li>
                        <li>Architectures for cloud, high-performance computing, and data centers.</li>
                        <li>Next-generation memory architectures.</li>
                        <li>Parallel architectures.</li>
                        <li>Power‚Äêefficient architectures and systems.</li>
                        <li>Embedded, IoT, reconfigurable, and heterogeneous architectures.</li>
                        <li>Architectures for emerging technology and applications.</li>
                        <li>Quantum computing, quantum circuit optimization.</li>
                    </ul>
                    
                    <br>

                    <!--
                    <h3 class="section-title">Past Editions</h3>
                        
                    <ul class="list-arrow-right">
                        <li><a href="https://augustojv.github.io/foca-workshop-2021" target="_blank">2021</a></li>
                        <li><a href="https://augustojv.github.io/foca-workshop-2020" target="_blank">2020</a></li>
                        <li><a href="https://augustojv.github.io/foca-workshop-2019" target="_blank">2019</a></li>
                        <li><a href="https://researcher.watson.ibm.com/researcher/view_group.php?id=9671" target="_blank">2018</a></li>
                        <li><a href="https://researcher.watson.ibm.com/researcher/view_group.php?id=8187" target="_blank">2017</a></li>
                        <li><a href="http://researcher.watson.ibm.com/researcher/view_group.php?id=7424" target="_blank">2016</a></li>
                    </ul>
                    -->

                    <br>
                    
                    <h3 class="section-title">Contact</h3>
                        
                    <ul class="list-arrow-right">
                    <li><a href="mailto:info@foca-workshop.org">info@foca-workshop.org</a></li> 
                    </ul>
                </div><!-- /.col-sm-6 -->

                <div class="col-sm-6">

                    <h3 class="section-title multiple-title">Organizing Committee</h3>
                    <p>
					<ul class="list-arrow-right">
                        <li>Augusto Vega (IBM)</li>
                        <li>Chris Wilkerson (Intel)</li>
                        <li>David Trilla (IBM)</li>
                        <li>Jennifer Dworak (SMU)</li>
                        <li>Karthik Swaminathan (IBM)</li>
                        <li>Nathan Manohar (IBM)</li>
                        <li>Pradip Bose (IBM)</li>
                        <li>Subhashish Mitra (Stanford)</li>
                        <li>Ramon Bertran (IBM)</li>
					</ul>
                    </p>

                </div><!-- /.col-sm-6 -->
            </div><!-- /.row -->
        </div><!-- /.container -->
    </section>

    <section id="links" class="section bg-image-2 facts ">
        <div class="container">
            <div class="row">
				<div class="col-md-12">
                    <h3 class="section-title">Interesting Links</h3>
                </div>
				<div class="col-md-3">
                    <p>The <a href="http://www.computerhistory.org/timeline/ai-robotics/" rel="nofollow" target="_blank">AI and Robotics Timeline</a> from 1939 to date.</p>
                </div><div class="col-md-3">
                    <p>The <a href="https://www.research.ibm.com/ibm-q/technology/experience/" rel="nofollow" 
                        target="_blank">IBM Q Experience</a> to try a quantum computer online.</p>
                </div><div class="col-md-3">
                    <p>IBM Watson in action in this <a href="https://visual-recognition-demo.mybluemix.net" rel="nofollow" target="_blank">on-line demo</a> using deep learning.</p>
                </div><div class="col-md-3">
                    <p>The <a href="http://research.ibm.com/cognitive-computing/" rel="nofollow" target="_blank">AI Portal</a> with the latest IBM research activities.</p>
                </div>
          </div>
        </div>
    </section>
    
    <section id="speakers" class="section speakers">
        <div class="container">
            <div class="row">
              <div class="col-md-12">
                <h3 class="section-title">Keynote Speakers</h3>
                TBA
              </div>
            </div>
            <!-- 
            <div class="row">
              <div class="col-md-2">
                  <div class="speaker">
                  <figure>
                    <img alt="" class="img-responsive center-block" src="assets/images/speakers/placeholder.png" data-content="Andy will talk about the Mayflower Autonomous Ship project, in which IBM is the technology partner. Mayflower has no captain and no crew, and is a scientific research vessel for gathering data from the ocean to help us understand the impact of climate change and pollution, studying things like microplastics and the health of marine mammals. This fully autonomous boat is scheduled to sail from Plymouth UK to Plymouth Massachusetts, recreating the journey of the Pilgrim Fathers just over 400 years ago. Andy will describe how the AI Captain navigates the ship and talks about some of the scientific experiments on board, and the data these are producing.">
                  </figure>
                </div>
             </div>
             <div class="col-sm-10">
                    <h3>Mayflower Autonomous Ship - the future of autonomous marine exploration</h3>
                    <h4>Andy Stanford-Clark (IBM Corporate Strategy)</h4>
                    <p>Prof Andy Stanford-Clark leads transformational innovation projects for clients in EMEA as part of IBM's Corporate Strategy organisation. He is an IBM Distinguished Engineer, a Master Inventor with more than 40 patents, and is IBM's Quantum Computing leader for the UK. Andy is based at IBM's Hursley Park laboratories near Winchester, and has has been working in the area that we now call the Internet of Things for more than 20 years. He has a BSc in Computing and Mathematics, and a PhD in Computer Science. He is a Visiting Professor at the University of Newcastle, an Honorary Professor at the University of East Anglia, an Adjunct Professor at the University of Southampton, and a Fellow of the British Computer Society.</p>
            </div>
             </div>
          <div class="row">
              <div class="col-md-2">
                  <div class="speaker">
                  <figure>
                    <img alt="" class="img-responsive center-block" src="assets/images/speakers/placeholder.png" data-content="Reports of exploits and vulnerabilities at every layer of the systems stack are reflective of a growing trend of increasingly sophisticated attackers and an urgent need for more innovative and effective defenses. Attackers have seized the advantage with the democratization of AI and the ubiquitous availability of computing resources via third party service providers while defenders endeavor to keep pace. The recent Presidential Executive Order on Improving the Nation's Cybersecurity from the current US administration significantly raises the bar on security on both government and commercial IT service providers creating a unique opportunity to infuse new security architectures and technologies into enterprise defenses. The time is ripe for harvesting decades long work in systems security research to implement and deploy confidential computing mechanisms to lay the foundation for zero trust architectures and secure enterprise and cloud environments. In this talk, we will describe the drivers and evolution of both compute-centric and data-centric confidential computing mechansims that were pioneered at IBM Research and discuss the important problems that remain to be solved to realize the promise of these technologies and bring change across the industry.">
                  </figure>
                </div>
             </div>
             <div class="col-sm-10">
               <h3>Systems Security in a Zero Trust World: An Industrial Research Perspective</h3>
               <h4>Josyula R. Rao</h4>
               <p>J.R. Rao is an IBM Fellow and CTO for the Security Research team at IBM. Based at the IBM Thomas. J. Watson Research Center, the global team comprises more than 200 researchers who work in the areas of AI Security, Cybersecurity, Cloud and Systems Security, Information Security and Cryptography. JR works closely with commercial and government customers, academic partners and IBM business units to drive new and innovative technologies into IBM's products and services and definitive industry standards. The goal of his research is to significantly raise the bar on the quality of security while simultaneously easing the overhead of developing and deploying secure solutions. Dr. Rao has published widely in premier security conferences and workshops and holds numerous US and European patents. He is a member of the IBM Academy of Technology, emeritus member of IFIP's Working Group 2.3 (Programming Methodology) and the Industry Advisory Board of the Georgia Tech Information Security Center. Dr. Rao obtained his doctorate degree from the University of Texas at Austin, a Master‚Äôs degree in Computer Science from the State University of New York at Stony Brook, and a Bachelor of Technology degree in Electrical Engineering from the Indian Institute of Technology, Kanpur.</p>
            </div>
             </div>
            -->
      </div>
    </section>

    <section id="speakers" class="section speakers">
        <div class="container">
            <div class="row">
                <div class="col-md-12">
                    <h3 class="section-title">Invited Speakers</h3>
                </div>
            </div>
            TBA
            <!--
            <div class="row">
                <div class="col-md-3">
                    <div class="speaker">
                        <figure>
                            <img alt="" title="Bio" class="img-responsive center-block" src="assets/images/speakers/placeholder.png" >
                        </figure>
                        <h4>Joseph Zuckerman</h4>
                        <p>Columbia University</p> 
                    </div>
                </div>
                <div class="col-md-3">
                    <div class="speaker">
                        <figure>
                            <img alt=""  title="Bio" class="img-responsive center-block" src="assets/images/speakers/placeholder.png" data-content="">
                        </figure>
                        <h4>Helena Caminal</h4>
                        <p>Cornell University</p> 
                    </div>
                </div>
                <div class="col-md-3">
                    <div class="speaker">
                        <figure>
                            <img alt=""  title="Bio" class="img-responsive center-block" src="assets/images/speakers/placeholder.png" data-content="">
                        </figure>
                        <h4>Maitreyi Ashok</h4>
                        <p>Massachusetts Institute of Technology</p> 
                    </div>
                </div>
            </div>
            <div class="row">
                <div class="col-md-3 ">
                    <div class="speaker">
                        <figure>
                            <img alt=""  title="Bio" class="img-responsive center-block" src="assets/images/speakers/placeholder.png" data-content="">
                        </figure>
                        <h4>Gauri Patwardhan</h4>
                        <p>University of Illinois Urbana-Champaign</p> 
                    </div>
                </div>
                <div class="col-md-3">
                    <div class="speaker">
                        <figure>
                            <img alt=""  title="Bio" class="img-responsive center-block" src="assets/images/speakers/placeholder.png" data-content="">
                        </figure>
                        <h4>Meghan Clark</h4>
                        <p>University of California, Berkeley</p>
                    </div>
                </div>
                <div class="col-md-3">
                    <div class="speaker">
                        <figure>
                            <img alt=""  title="Bio" class="img-responsive center-block" src="assets/images/speakers/placeholder.png" data-content="">
                        </figure>
                        <h4>Vikram Narayanan</h4>
                        <p>University of California, Irvine</p>
                    </div>
                </div>
            </div>
            <div class="row">
                <div class="col-md-4 col-md-offset-1">
                    <div class="speaker">
                        <figure>
                            <img alt=""  title="Bio" class="img-responsive center-block" src="assets/images/speakers/placeholder.png" data-content="">
                        </figure>
                        <h4>Jovan Stojkovic</h4>
                        <p>University of Illinois Urbana-Champaign</p>
                    </div>
                </div>
                <div class="col-md-3">
                    <div class="speaker">
                        <figure>
                            <img alt="" title="Bio"  class="img-responsive center-block" src="assets/images/speakers/placeholder.png" data-content="">
                        </figure>
                        <h4>Poulami Das</h4>
                        <p>Georgia Institute of Technology</p> 
                    </div>
                </div>
            </div>
            -->
        </div>
    </section>

    <section id="program" class="section schedule">

        <div class="container">
            <div class="row">
                <div class="col-md-11">
                    <h3 class="section-title">Program</h3>
                    TBA
                    (<em>all times are in Eastern Time</em>)
                    <p>
                    <!--
                    <table class="table table-hover">
                      <thead class="thead-light">
                        <tr>
                          <th colspan=2 scope="col">October 27th<sup></sup>, 2022</th>
                        </tr>
                      </thead>
                      <tbody>
                        <tr>
                          <th scope="row">8:50 - 9:00am</th>
                          <td>Introduction and Welcoming Remarks</td>
                        </tr>
                        <tr bgcolor="#EBF5FB">
                          <th scope="row">9:00 - 09:45am</th>
                          <td data-toggle="popover" data-placement="top" data-container="body" data-content="Andy will talk about the Mayflower Autonomous Ship project, in which IBM is the technology partner. Mayflower has no captain and no crew, and is a scientific research vessel for gathering data from the ocean to help us understand the impact of climate change and pollution, studying things like microplastics and the health of marine mammals. This fully autonomous boat is scheduled to sail from Plymouth UK to Plymouth Massachusetts, recreating the journey of the Pilgrim Fathers just over 400 years ago. Andy will describe how the AI Captain navigates the ship and talks about some of the scientific experiments on board, and the data these are producing.">
                            <strong>Keynote: Mayflower Autonomous Ship - the future of autonomous marine exploration </strong> <br>
                            Andy Stanford-Clark (IBM Corporate Strategy) </br> </td>
                        </tr>
                        <tr bgcolor="#FEF9E7">
                          <th scope="row">09:45 - 10:00am</th>
                          <td><i>Break</i></td>
                        </tr>
                        <tr>
                          <th scope="row">10:00 - 10:30am</th>
                          <td data-toggle="popover" data-placement="top" data-container="body" data-content="Data processing requirements of big data and machine learning applications are outstripping the data transfer capacity of classic Von Neumann architectures. The main area where the current computing paradigm and architecture have lagged is data movement. The transfer of data from where it is stored, the off-chip memory, to where it is analyzed, the processor, takes one order of magnitude more time and consumes two orders of magnitude higher energy than the actual computation (e.g., a single-precision addition). Adding processing units as close as possible to the memory cells, i.e., processing in memory (PIM), alleviates the data movement overhead, unlocking the value of modern applications. However, realizing the true potential of PIM requires rethinking and simplifying the control, access, and communication mechanisms of PIM units. In my talk, I first discuss the inefficacy of traditional MIMD, SIMD, and SIMT architectures for data-intensive applications, with few computations per data element and challenges due to control and access divergence. Then, I introduce our proposed architectures, Fulcrum and FulcrumV2 (Gearbox), that offer a trade-off between (i) full control and access divergence support in MIMD and (ii) no/costly control or divergence support in SIMD/SIMT approaches. We show the flexibility of our design by mapping important kernels from different domains such as machine learning, database management, and graph processing. Finally, I conclude with our vision for identifying common requirements of data-intensive applications, designing a semi-general-purpose PIM-based accelerator (which supports a wide range of applications), and developing a software stack for our accelerator."> Rethinking Control, Access, and Communication Mechanisms for Data-intensive Applications <br> Marzieh Lenjani ( University of Virginia ) </td> 
                        </tr>
                         -->
                         <!--
                        <tr>
                          <th scope="row">10:30 - 11:00am</th>
                          <td data-toggle="popover" data-placement="top" data-container="body" data-content="This talk will focus on two broad areas of the research performed by the speaker on efficient DL accelerator design. Designing efficient custom architecture is a costly endeavor, especially in the face of changing workload requirements, e.g., DL workloads. These workloads are often becoming computing-intensive and irregular, rendering the existing acceleration platforms suboptimal and inefficient and prompting new architectural exploration. The first area of the talk will focus on efficient architecture design space exploration for building performant, systolic array-based DL accelerators for specific workloads and design constraints. Specifically, the speaker will present analytical models and the SCALE-Sim simulator to demonstrate their utility in systematically exploring the design space to find the optimal design configurations. In the second half of the talk, the speaker will present recent and ongoing work on leveraging AI techniques to learn the design and mapping space of DNN accelerators. This portion of the talk will demonstrate formulating architecture design space exploration as an ML problem and learning the optimization space to directly predict optimal architecture configuration without search, thus reducing the non-recurring engineering costs when optimizing architecture for new workloads."> AIrchitect: Leveraging AI to build scalable and flexible Deep Learning accelerators <br> Ananda Samajdar ( Georgia Institute of Technology ) </td> 
                        </tr>
                        <tr>
                          <th scope="row">11:00 - 11:30am</th>
                          <td data-toggle="popover" data-placement="top" data-container="body" data-content="Deep Neural Networks (DNNs) have witnessed widespread success in several machine learning tasks, leading to their deployment in several real-world products and services. This success has been enabled by advancements in the design of hardware platforms such as Graphics Processing Units (GPUs) and accelerators. However, recent trends in state-of-the-art DNNs point to enormous and ever-increasing compute requirements, surpassing the rate of advancements in deep learning hardware. This growing gap between the compute demands and hardware capabilities threatens to stymie progress in the field of machine learning. Creating the next generation of efficient and robust machine learning systems will require hardware-aware design, or consideration of hardware characteristics in the design of the algorithms. My talk broadly characterizes our efforts into two directions, i.e., improving execution efficiency and robustness of  machine learning systems. As part of the first direction of efforts, I will be discussing techniques to improve the training complexity of DNNs through hardware awareness. We first consider the widely-used stochastic gradient descent (SGD) algorithm used for training DNNs. We propose a method to use localized learning, which is computationally cheaper and incurs lower memory footprint, to accelerate an SGD-based training framework with minimal impact on accuracy. This is achieved by employing localized learning in a spatio-temporally selective manner, i.e., in selected network layers and epochs. Next, we consider input interpolation techniques to reduce the effective size of the training dataset every epoch. Specifically, groups of training inputs are combined such that training on the resulting composite sample, has a similar effect on model performance as training on the individual inputs. To preserve accuracy, we propose techniques to mitigate the interference between the features of the constituent inputs in the composite sample. Further, we devise selective interpolation strategies, i.e., only a subset of training inputs are interpolated every epoch. For the second direction of our efforts on robustness, I will be discussing a new challenge identified in the field of adversarial attacks, by proposing attacks that degrade the execution efficiency (energy or time) of a DNN on a given hardware platform. As a specific embodiment of such attacks, we propose sparsity attacks, which perturb the inputs to a DNN so as to result in much lower sparsity within the network, causing it‚Äôs latency and energy to be increased on sparsity-optimized hardware platforms. In summary, this talk will demonstrate that hardware-awareness can result in improved performance of deep learning, while also opening up new classes of attacks that must be addressed."> Hardware-Aware Efficient and Robust Deep Learning <br> Sarada Krithivasan ( Purdue Univers 
                        </tr>
                        <tr bgcolor="#FEF9E7">
                          <th scope="row">11:30 - 01:30pm</th>
                          <td><i>Lunch Break</i></td>
                        </tr>
                        <tr bgcolor="#EBF5FB">
                          <th scope="row">01:30 - 02:15pm</th>
                          <td data-toggle="popover" data-placement="top" data-container="body" data-content="Reports of exploits and vulnerabilities at every layer of the systems stack are reflective of a growing trend of increasingly sophisticated attackers and an urgent need for more innovative and effective defenses. Attackers have seized the advantage with the democratization of AI and the ubiquitous availability of computing resources via third party service providers while defenders endeavor to keep pace. The recent Presidential Executive Order on Improving the Nation's Cybersecurity from the current US administration significantly raises the bar on security on both government and commercial IT service providers creating a unique opportunity to infuse new security architectures and technologies into enterprise defenses. The time is ripe for harvesting decades long work in systems security research to implement and deploy confidential computing mechanisms to lay the foundation for zero trust architectures and secure enterprise and cloud environments. In this talk, we will describe the drivers and evolution of both compute-centric and data-centric confidential computing mechansims that were pioneered at IBM Research and discuss the important problems that remain to be solved to realize the promise of these technologies and bring change across the industry.">
                            <strong>Keynote: Systems Security in a Zero Trust World: An Industrial Research Perspective</strong> <br> Josyula R. Rao (IBM Research) </br> </td>
                        </tr>
                        <tr>
                          <th scope="row">02:15 - 02:30pm</th>
                          <td><i>Break</i></td>
                        </tr>
                        <tr>
                          <th scope="row">02:30 - 03:00pm</th>
                          <td data-toggle="popover" data-placement="top" data-container="body" data-content="Deep Neural Networks (DNNs) achieve state-of-the-art performance in a variety of machine learning tasks such as computer vision and natural language processing. The remarkable algorithmic performance of DNNs comes with extremely high computation and storage demands, which outpace the growth in capabilities of commodity hardware. In this talk, I will present our two recent efforts to address this challenge ‚Äì (i) Approximate computation method to design efficient precision-reconfigurable hardware and (ii) In-Memory computation framework enabled by compact and energy-efficient Piezoelectric FETs (PeFETs). Precision scaling is a popular technique to minimize both the compute and storage requirements of DNNs. Efforts toward creating ultra-low-precision (sub-8-bit) DNNs for efficient inference suggest that the minimum precision required to achieve a given network-level accuracy varies considerably across and within networks, requiring support for variable precision in DNN hardware. Previous proposals such as bit-serial hardware incur high overheads, significantly diminishing the benefits of lower precision. To efficiently support precision re-configurability in DNN accelerators, we introduce an approximate computing method wherein multiply-accumulate computations are performed at the granularity of blocks (a block is a group of bits). We propose approximations to block-wise computation in order to enable precision reconfigurability with low overheads. We show that our framework can achieve significant improvements in system energy and performance with small loss in classification accuracy. The performance and energy-efficiency of data-intensive applications like DNN inference in traditional Von-Neumann architectures are bottlenecked by the slow and energy-intensive data movements between memory and processing unit. In-memory computing is a promising approach that alleviates this bottleneck by computing within memory. Non-volatile memory (NVM) based in-memory computing has gained considerable interest recently due to the desirable characteristics of NVMs such as high density and low-leakage power. Most of the previously proposed NVMs have a major disadvantage ‚Äì slow and power-hungry current-driven write. Recently, Piezoelectric FETs (PeFETs) have shown considerable potential for efficient future memory hierarchies due to their fast and low-power electric-field driven write. We enhance the PeFET memory cell with ternary compute capability to perform efficient in-memory computation. We demonstrate the benefits of the ternary compute-enabled PeFET cell by performing ternary DNN inference and achieving significant system improvements compared to a well-optimized near-memory SRAM baseline."> Precision Reconfigurable and In-Memory Computing Architectures for Efficient DNN Inference <br> Reena Elangovan ( Purdue University ) </td> 
                        </tr>
                        <!--
                        <tr>
                          <th scope="row">03:00 - 03:30pm</th>
                          <td data-toggle="popover" data-placement="top" data-container="body" data-content="Graph clustering and community detection are central problems in modern data mining. The increasing need for analyzing billion-scale data  calls for faster and more scalable algorithms for these problems. There are certain trade-offs between the quality and speed of such clustering algorithms. In this paper, we design scalable algorithms that achieve high quality when evaluated based on ground truth. We develop a generalized sequential and shared-memory parallel framework based on the LambdaCC objective (introduced by Veldt et al.), which encompasses modularity and correlation clustering. Our framework consists of highly-optimized implementations that scale to large data sets of billions of edges and that obtain high-quality clusters compared to ground-truth data, on both unweighted and weighted graphs. Our empirical evaluation shows that this framework improves the state-of-the-art trade-offs between speed and quality of scalable community detection. For example, on a 30-core machine with two-way hyper-threading, our implementations achieve orders of magnitude speedups over other correlation clustering baselines, and up to 28.44x speedups over our own sequential baselines while maintaining or improving quality. "> Scalable Community Detection via Parallel Correlation Clustering <br> Jessica Shi ( Massachusetts Institute of Technology ) </td> 
                        </tr>
                        <tr>
                          <th scope="row">03:30 - 04:00pm</th>
                          <td data-toggle="popover" data-placement="top" data-container="body" data-content="Distributed applications such as key-value stores and databases avoid frequent writes to secondary storage devices to minimize performance degradation. They provide fault tolerance by replicating variables in the memories of different nodes, and using data consistency protocols to ensure consistency across replicas. Unfortunately, the reduced data durability guarantees provided can cause data loss or slow data recovery. In this environment, non-volatile memory (NVM) offers the ability to attain both high performance and data durability in distributed applications. However, it is unclear how to tie NVM memory persistency models to the existing data consistency frameworks, and what are the durability guarantees that the combination will offer to distributed applications. In this presentation, I‚Äôll talk about the concept of Distributed Data Persistency (DDP) model, which is the binding of the memory persistency model with the data consistency model in a distributed system. I reason about the interaction between consistency and persistency by using the concepts of Visibility Point and Durability Point. I present the design of low-latency distributed protocols for DDP models that combine five consistency models with five persistency models. For the resulting DDP models, I‚Äôll show the trade-offs between performance, durability, and intuition provided to the programmer."> Distributed Data Persistency <br> Apostolos Kokolis ( University of Illinois Urbana-Champaign ) </td> 
                        </tr>
                        <tr bgcolor="#FEF9E7">
                          <th scope="row">04:00 - 04:15pm</th>
                          <td><i>Break</i></td>
                        </tr>						
                        <tr>
                          <th scope="row">04:15 - 04:45pm</th>
                          <td data-toggle="popover" data-placement="top" data-container="body" data-content="The end of conventional processor scaling has driven research and industry practice to explore a new generation of approaches with rich features and optimizations opportunities especially in the sparse computation domains. One such domain is Graphs which are ubiquitous and taking advantage of the right features is the key to performance. We propose the Unified Graph Framework (UGF) that extends the GraphIt DSL's optimization and code generation techniques to different architectures. UGF achieves portability with reasonable effort by decoupling the architecture-independent algorithm from the architecture-specific backend and schedules. We introduce a new domain-specific intermediate representation, the GraphIR, that is key to this decoupling. The GraphIR encodes high-level algorithm and optimization information needed for hardware-specific code generation, making it easy to develop different backends (GraphVMs) for diverse architectures, including CPUs, GPUs, and next generation hardware such as Swarm and the HammerBlade manycore. Our evaluations show UGF allows optimizations that can improve performance up-to 53x over the default generated code across 5 applications, 9 graphs and 4 architectures. For the GPU GraphVM, UGF generated code is upto 5.11x faster than the next fastest state-of-the-art framework."> Taming the Zoo: A Unified Graph Compiler Framework for Novel Architectures <br> Ajay Brahmakshatriya ( Massachusetts Institute of Technology ) </td> 
                        </tr>
                    -->
                    <!--
                        <tr>
                          <th scope="row">04:45 - 05:15pm</th>
                          <td data-toggle="popover" data-placement="top" data-container="body" data-content="Microarchitectural enhancements that improve performance generally, across many workloads, are favored in superscalar processor design. Targeting general performance is necessary but it is also constraining. We explore relieving this constraint, via a new paradigm called Post-Fabrication Microarchitecture (PFM).  A high-performance superscalar core is coupled with a reconfigurable logic fabric, RF. A programmable interface, or Agent, allows for RF to observe and microarchitecturally intervene at key pipeline stages of the superscalar core. New microarchitectural components, specific to applications, are synthesized on-demand to RF. All instructions still flow through the superscalar pipeline, as usual, but their execution is streamlined (better instructions per cycle (IPC)) through microarchitectural intervention by RF. Our research shows that one can achieve large speedups of individual applications, by analyzing their bottlenecks and providing customized microarchitectural solutions to target these bottlenecks. Examples of PFM use-cases explored in this paper include custom branch predictors and data prefetchers."> Post-Fabrication Microarchitecture <br> Anirudh Seshadri ( North Carolina State University ) </td> 
                        </tr>
                        <tr>
                          <th scope="row">05:15 - 05:45pm</th>
                          <td data-toggle="popover" data-placement="top" data-container="body" data-content="Maintaining a k-core decomposition quickly in a dynamic graph is an important problem in many applications, including social network analytics, graph visualization, centrality measure computations, and community detection algorithms. The main challenge for designing efficient k-core algorithms is that a single change to the graph can cause the decomposition to change significantly. We present the first parallel batch-dynamic algorithm for maintaining an approximate k-core decomposition that is efficient in both theory and practice. Given an initial graph with m edges, and a batch of B updates, our algorithm provably maintains a (2 + c)-approximation of the coreness values for all vertices (for any constant c > 0) in O(B log^2 m) amortized work and O(log^2 m log log m) depth (parallel time) with high probability. We present the first proof of this approximation factor in the dynamic setting under our depth and work bounds in the batch-dynamic setting. We implemented and experimentally evaluated our algorithm on a 30-core machine with two-way hyper-threading on 11 graphs of varying densities and sizes. Compared to the state-of-the-art algorithms, our algorithm achieves up to a 114.52x speedup against the best parallel implementation, up to a 544.22x speedup against the best approximate sequential algorithm, and up to a 723.72x speedup against the best exact sequential algorithm. We also obtain results for our algorithms on graphs that are orders-of-magnitude larger than those used in previous studies.

"> Fast and Scalable Parallel Batch-Dynamic k-Core Decomposition <br> Quanquan Liu ( Massachusetts Institute of Technology ) </td> 
                        </tr>
                        <tr>
                          <th scope="row">05:45</th>
                          <td>Concluding Remarks</td>
                        </tr>
                      </tbody> 
                    </table>
                    -->
                    </p>
                </div>
            </div>
    </section>

    <section id="org" class="section registration">
        <div class="container">
            <div class="row">
                <div class="col-md-12">
                    <h3 class="section-title">Organizers</h3>
                </div>
                <div class="col-md-8">
                <p>
                <b><a href="http://researcher.watson.ibm.com/researcher/view.php?person=us-kvswamin" rel="nofollow" 
                    target="_blank">Karthik Swaminathan</a> </b> is a research staff member at the Efficient and Resilient Systems Group at the IBM T.J Watson Research Center. His research has a broad, cross-layer scope examining circuit, architecture and application level optimizations for improving the reliability and energy efficiency of multi core systems and accelerators.  He also works on characterizing performance and reliability of IBM server-class and mainframe processors at various stages of design. He holds a PhD from Penn State University. 
                </p>
                <p>
                <b>David Trilla</b> is a Post-doctoral Researcher at IBM T. J. Watson Research Center. He has worked on real-time systems and current 
                research interests include security and agile hardware development. He obtained his Ph.D. at the Barcelona Supercomputing Center (BSC) granted by 
                the Polytechnic University of Catalonia (UPC), Spain.
                </p>
                <p>
                <b><a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-ajvega" rel="nofollow" target="_blank">Augusto 
                    Vega</a> </b>is a Research Staff Member at IBM T. J. Watson Research Center involved in research and development work in 
                    the areas of highly-reliable power-efficient embedded designs, cognitive systems and mobile computing. He holds M.S. and 
                    Ph.D. degrees from Polytechnic University of Catalonia (UPC), Spain.
                </p>
                <!-- <p>
                <b><a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-xque" rel="nofollow" target="_blank">Xinyu 
                    Que</a> </b> is a Research Staff Member in the Data Centric Systems Co-Design department at the T. J. Watson Research 
                    Center. He received his M.S. degree in Computer Science and Engineering from University of Connecticut and Ph.D. 
                    degrees in Computational Science and Software Engineering from Auburn University. Xinyu has broad interests in high 
                    performance computing and large-scale graph analytics.
                </p> -->
                </div>
            </div>
        </div>
    </section>

    <footer class="site-footer facts">
        <div class="container">
            <div class="row">
                <div class="col-md-12">
                    <p class="site-info">Share this event on your social networks</p>
                </div>
                <div class="col-md-4">
                    <a href="https://twitter.com/home?status=Check%20out%20the%20Workshop%20on%20the%20Future%20of%20Computing%20Architectures%0Ahttps%3A//foca-workshop.org" rel="nofollow" target="_blank"><i class="ion-social-twitter"></i>&nbsp; Twitter</a>
                </div>
                <div class="col-md-4">
                    <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A//foca-workshop.org/" rel="nofollow" target="_blank"><i class="ion-social-facebook"></i>&nbsp; Facebook</a>
                </div>
                <div class="col-md-4">
                    <a href="https://www.linkedin.com/shareArticle?mini=true&url=https%3A//foca-workshop.org/&title=Workshop%20on%20the%20Future%20of%20Computing%20Architectures&summary=&source=" rel="nofollow" target="_blank"><i class="ion-social-linkedin-outline"></i>&nbsp; LinkedIn</a>
                </div>
            </div>
        </div>
    </footer>

    <!-- script -->
    <script src="bower_components/jquery/dist/jquery.min.js"></script>
    <script src="bower_components/bootstrap/dist/js/bootstrap.min.js"></script>
    <script src="bower_components/smooth-scroll/dist/js/smooth-scroll.min.js"></script>
    <script src="assets/js/main.js"></script>
	<script>
	$(document).ready(function(){
		$('[data-toggle="popover"]').popover({
			trigger : 'hover',
			title: 'Abstract',
			html    : true
		});		
	});
	</script>
</body>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-88908704-1', 'auto');
  ga('send', 'pageview');

</script>
  
</html>
